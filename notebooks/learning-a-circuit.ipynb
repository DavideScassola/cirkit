{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e6d6b9",
   "metadata": {},
   "source": [
    "# Learning a Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf2b04-012a-4d5f-8700-39e848f6b2c9",
   "metadata": {},
   "source": [
    "TODO: explain what are we going to do (high level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6debe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c005fb",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02baea",
   "metadata": {},
   "source": [
    "TODO: stress we can use any library to load data sets, and everything will work\n",
    "\n",
    "Load the training and test splits of MNIST, and preprocess them by flattening the tensor images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a04559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "num_variables = data_train[0][0].shape[0]\n",
    "height, width = 28, 28\n",
    "print(f\"Number of variables: {num_variables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d0122",
   "metadata": {},
   "source": [
    "## Instantiating a Circuit structure Template: the Region Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d471b6e",
   "metadata": {},
   "source": [
    "TODO: explain what a region graph is + add citation\n",
    "\n",
    "Initialize a _Quad Tree_ region graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates.region_graph import QuadTree\n",
    "region_graph = QuadTree(shape=(height, width), num_patch_splits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66900897-5d65-4136-8399-f997c3665a38",
   "metadata": {},
   "source": [
    "## Constructing the Symbolic Circuit Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6468f6-a1e5-46f8-92d4-d1eb6fa5ba19",
   "metadata": {},
   "source": [
    "TODO: refactor this, explain symbolic circuits as an intermediate representation before compilation (link section)\n",
    "\n",
    "From the region graph definition above, we now construct the symbolic circuit representation. Note that this circuit representation is _not_ executable, i.e., you cannot do learn it or do inference with it. It will be compiled later, by choosing a backend such as torch.\n",
    "\n",
    "To do so, we first define the factories that will be used to construct symbolic layers. Note that we choose the parameterization at the symbolic level. That is, we guarantee non-negative parameters by passing them through an exponential function. Moreover, we can choose how to parameterize the categorical distributions used to model the distribution of pixel values in the 0-255 range. In this case, we use a log softmax function. We choose to initialize the weights of the circuit by sampling from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a1895-999f-47af-8c6f-57242726540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.utils.scope import Scope\n",
    "from cirkit.symbolic.parameters import SoftmaxParameter, ExpParameter, TensorParameter, Parameter\n",
    "from cirkit.symbolic.layers import CategoricalLayer, DenseLayer, KroneckerLayer, MixingLayer\n",
    "from cirkit.symbolic.initializers import NormalInitializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b9dfc-83a6-43ba-b20d-48d6dd79e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove sum/product/mixing factories and use the higher level APIs\n",
    "\n",
    "\n",
    "# TODO: Remove categorical factory, Add string option in the lib\n",
    "def categorical_layer_factory(\n",
    "    scope: Scope,\n",
    "    num_units: int,\n",
    "    num_channels: int\n",
    ") -> CategoricalLayer:\n",
    "    return CategoricalLayer(\n",
    "        scope, num_units, num_channels, num_categories=256,\n",
    "        probs_factory=lambda shape: Parameter.from_unary(\n",
    "            SoftmaxParameter(shape),\n",
    "            TensorParameter(*shape, initializer=NormalInitializer(0.0, 1e-2))\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def kronecker_layer_factory(\n",
    "    scope: Scope, num_input_units: int, arity: int\n",
    ") -> KroneckerLayer:\n",
    "    return KroneckerLayer(scope, num_input_units, arity)\n",
    "\n",
    "\n",
    "def dense_layer_factory(\n",
    "    scope: Scope,\n",
    "    num_input_units: int,\n",
    "    num_output_units: int\n",
    ") -> DenseLayer:\n",
    "    return DenseLayer(\n",
    "        scope, num_input_units, num_output_units,\n",
    "        weight_factory=lambda shape: Parameter.from_unary(\n",
    "            ExpParameter(shape),\n",
    "            TensorParameter(*shape, initializer=NormalInitializer(0.0, 1e-1))\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def mixing_layer_factory(\n",
    "    scope: Scope, num_units: int, arity: int\n",
    ") -> MixingLayer:\n",
    "    return MixingLayer(\n",
    "        scope, num_units, arity,\n",
    "        weight_factory=lambda shape: Parameter.from_unary(\n",
    "            ExpParameter(shape),\n",
    "            TensorParameter(*shape, initializer=NormalInitializer(0.0, 1e-1))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9707de19-d6c7-4646-8266-2e20dcefa22e",
   "metadata": {},
   "source": [
    "Then, we call a function to construct the symbolic circuit from region graph, by specifying the number of units and the factories to build layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be2b8f-0012-46ed-9f03-26a7a4729b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.symbolic.circuit import Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c88f38-1552-4d13-b62d-931493c07c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this\n",
    "\n",
    "symbolic_circuit = templates.image_data(\n",
    "    image_shape=(1, 28, 28),\n",
    "    region_graph='quad-tree',\n",
    "    num_input_units=8\n",
    "    num_sum_units=8,\n",
    "    input_layer='categorical',   # TODO: implement this, rather than using the factory\n",
    "    sum_product='cp',\n",
    "    sum_weight_param='softmax'   # TODO: implement this instead of dense_weight_factory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c6e7c-ad9f-4dd2-ab76-602e191d197b",
   "metadata": {},
   "source": [
    "TODO: discuss structural properties\n",
    "\n",
    "We can retrieve some information about the circuit and its structural properties as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23570bfd-a64e-4e19-ba4c-30e489e9d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Smooth: {symbolic_circuit.is_smooth}')\n",
    "print(f'Decomposable: {symbolic_circuit.is_decomposable}')\n",
    "print(f'Number of variables: {symbolic_circuit.num_variables}')\n",
    "print(f'Number of channels per variable: {symbolic_circuit.num_channels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67398c38",
   "metadata": {},
   "source": [
    "## Compiling the Symbolic Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba547d8",
   "metadata": {},
   "source": [
    "TODO: explain compilation procedure, we choose the torch backend\n",
    "\n",
    "We are ready to compile the symbolic circuit constructed above into another one that we can learn and/or do inference. To do so, we have to choose a compilation backend. In this case, we choose torch as a backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea4a4a-649d-462f-bcfd-20a12bd8a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: set seed also in random/numpy\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda')  # The device to use\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7a3f9-1c64-4117-81f0-8766ccbd3179",
   "metadata": {},
   "source": [
    "We first need to instantiate a circuit pipeline context and specify the backend to be used, as well as optional compilation flags, e.g., whether to fold the circuit or which inference semiring to use. Finally, we use the pipeline context to compile the symbolic circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1892e-4a65-4759-bb3a-ccbe6f5e515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "circuit = compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd14baf-a29e-4773-84a7-417d8747f678",
   "metadata": {},
   "source": [
    "Note that the compilation step, comprising the folding optimization, required just a few seconds for a circuit with ~5000 layers and ~400M learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f0d55e-05c3-42ac-a63c-326f8446f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(symbolic_circuit.layers)))\n",
    "print(sum(p.numel() for p in circuit.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee1f04",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf4858",
   "metadata": {},
   "source": [
    "TODO: refactor this comment, stress the user can choose any optimizer\n",
    "\n",
    "We are now ready to learn the parameters and do inference First, we wrap our data into PyTorch data loaders by specifying the batch size. Then, we initialize any PyTorch optimizer, e.g. SGD with momentum in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02854883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256, drop_last=True, num_workers=4)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256, num_workers=4)\n",
    "optimizer = optim.Adam(circuit.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6683d-f7c0-4b70-afd1-912a90861c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move circuit to device\n",
    "circuit = circuit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: rewrite this loop, we do not need the partition function anymore here\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "num_epochs = 3\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        batch = batch.to(device).unsqueeze(dim=1)   # Add a channel dimension\n",
    "        log_output = circuit(batch)                 # Compute the log output of the circuit\n",
    "        log_pf = pf_circuit()                       # Compute the log partition function of the circuit\n",
    "        lls = log_output - log_pf                   # Compute the log-likelihood\n",
    "        loss = -torch.mean(lls)   # The loss is the negative average log-likelihood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 100 == 0:\n",
    "            print(f\"Step {step_idx}: Average NLL: {running_loss / (100 * len(batch)):.3f}\")\n",
    "            running_loss = 0.0\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Training time: {end_time - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efae6b",
   "metadata": {},
   "source": [
    "We then evaluate our model on test data by computing the average log-likelihood and bits per dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit.eval()\n",
    "pf_circuit.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_lls = 0.0\n",
    "    log_pf = pf_circuit()  # Compute the log partition function of the circuit (just once as we are evaluating)\n",
    "    for batch, _ in test_dataloader:\n",
    "        batch = batch.to(device).unsqueeze(dim=1)   # Add a channel dimension\n",
    "        log_output = circuit(batch)                 # Compute the log output of the circuit\n",
    "        lls = log_output - log_pf                   # Compute the log-likelihood\n",
    "        test_lls += lls.sum().item()\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (num_variables * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033c3f8-424e-4859-aab2-c3d5cdbf5d41",
   "metadata": {},
   "source": [
    "TODO: show people we can do marginals, use integrate in cirkit.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598676ee-f33a-4ec8-9b3e-4e155fd956af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
