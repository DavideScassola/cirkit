{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e6d6b9",
   "metadata": {},
   "source": [
    "# Learning and Evaluating a Probabilistic Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf2b04-012a-4d5f-8700-39e848f6b2c9",
   "metadata": {},
   "source": [
    "In [a previous notebook](../learning-a-gaussian-mixture-model) we show how to construct and learn a circuit with ```cirkit```, manually building the symbolic circuit by hand. However, this can get quite cumbersome once we have enough input features.\n",
    "\n",
    "In this notebook, we will see how to **leverage built-in functions** from ```cirkit``` to easily build a deep circuit with (potentially) million of parameters. In particular, we will fit such a model on the MNIST dataset, and see how we can evaluate it on unseen images, marginalize/condition some pixels, and sample new images.\n",
    "\n",
    "Next, we show how to construct a symbolic circuit whose structure and parameterization is tailored for images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66900897-5d65-4136-8399-f997c3665a38",
   "metadata": {},
   "source": [
    "## Constructing the Symbolic Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6468f6-a1e5-46f8-92d4-d1eb6fa5ba19",
   "metadata": {},
   "source": [
    "The **symbolic circuit** is a symbolic abstraction of a tensorized circuit. This representation tracks the layer connections, number of units per layer, and other useful metadata about the parameters, such as their shape and parameterization choices. \n",
    "\n",
    "We provide in ```cirkit.templates``` helper functions to build symbolic circuits with different structures. We will use one tailored for image data, providing some arguments that determine the shape and form of the circuit. \n",
    "\n",
    "For example, we choose _QuadGraph_ as our region graph which exploits the closeness of patches of pixels. See the [notebook on region graphs and sum product layers](../region-graphs-and-parametrisation) for more details about region graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52c88f38-1552-4d13-b62d-931493c07c69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:53.113003Z",
     "start_time": "2024-10-09T14:18:52.822643Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.templates import data_modalities, utils\n",
    "\n",
    "symbolic_circuit = data_modalities.image_data(\n",
    "    (1, 28, 28),                # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-graph',  # Select the structure of the circuit to follow the QuadGraph region graph\n",
    "    input_layer='categorical',  # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=64,         # Each input layer consists of 64 Categorical input units\n",
    "    sum_product_layer='cp',     # Use CP sum-product layers, i.e., alternate dense layers with Hadamard product layers\n",
    "    num_sum_units=64,           # Each dense sum layer consists of 64 sum units\n",
    "    sum_weight_param=utils.Parameterization(\n",
    "        activation='softmax',   # Parameterize the sum weights by using a softmax activation\n",
    "        initialization='normal' # Initialize the sum weights by sampling from a standard normal distribution\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c6e7c-ad9f-4dd2-ab76-602e191d197b",
   "metadata": {},
   "source": [
    "We can query some information regarding the symbolic circuit, such as the number of variables it is defined on, and which structural properties it does satisfy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23570bfd-a64e-4e19-ba4c-30e489e9d08d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:53.181658Z",
     "start_time": "2024-10-09T14:18:53.147512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 784\n",
      "\n",
      "Structural properties:\n",
      "  - Smoothness: True\n",
      "  - Decomposability: True\n",
      "  - Structured-decomposability: False\n"
     ]
    }
   ],
   "source": [
    "# Print some information\n",
    "print(f'Number of variables: {symbolic_circuit.num_variables}')\n",
    "print()\n",
    "\n",
    "# Print which structural properties the circuit satisfies\n",
    "print(f'Structural properties:')\n",
    "print(f'  - Smoothness: {symbolic_circuit.is_smooth}')\n",
    "print(f'  - Decomposability: {symbolic_circuit.is_decomposable}')\n",
    "print(f'  - Structured-decomposability: {symbolic_circuit.is_structured_decomposable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67398c38",
   "metadata": {},
   "source": [
    "## Compiling the Symbolic Circuit with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c10766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba547d8",
   "metadata": {},
   "source": [
    "Note that *a symbolic circuit does not allocate parameters and cannot be used for learning or inference*, we need to **compile** the symbolic circuit. \n",
    "\n",
    "By default, ```cirkit``` compiles symbolic circuits using PyTorch 2+. Namely, it yields a regular [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) representing a tensorized circuit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7a3f9-1c64-4117-81f0-8766ccbd3179",
   "metadata": {},
   "source": [
    "Next, we import and use the ```compile``` function from ```cirkit.pipeline```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af58c11e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:57.132288Z",
     "start_time": "2024-10-09T14:18:54.991124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.54 s, sys: 6.33 ms, total: 1.55 s\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from cirkit.pipeline import compile\n",
    "circuit = compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed82999-59f8-411d-8447-c486991293d7",
   "metadata": {},
   "source": [
    "Note that the compilation took a couple seconds for a circuit with >5700 layers and ~25M parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0a1892e-4a65-4759-bb3a-ccbe6f5e515c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:57.152272Z",
     "start_time": "2024-10-09T14:18:57.148418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 5725\n",
      "Number of learnable parameters: 25657730\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics\n",
    "num_layers = len(list(symbolic_circuit.layers))\n",
    "print(f\"Number of layers: {num_layers}\")\n",
    "num_parameters = sum(p.numel() for p in circuit.parameters())\n",
    "print(f\"Number of learnable parameters: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee1f04",
   "metadata": {},
   "source": [
    "## Learning a Probabilistic Circuit using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf4858",
   "metadata": {},
   "source": [
    "Learning the probabilistic circuit we have compiled above can be done _in the same way as any other neural network_ written using PyTorch.\n",
    "\n",
    "Next, we load MNIST with [torchvision](https://pytorch.org/vision/stable/index.html), and select one of the many optimizers available in PyTorch, such as [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02854883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:18:58.418158Z",
     "start_time": "2024-10-09T14:18:57.224536Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Load the MNIST data set and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Flatten the images and set pixel values in the [0-255] range\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "\n",
    "# Initialize a torch optimizer of your choice,\n",
    "#  e.g., Adam, by passing the parameters of the circuit\n",
    "optimizer = optim.Adam(circuit.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08fe19-607d-42dc-8ec7-3ab1b6110274",
   "metadata": {},
   "source": [
    "Finally, we write a basic training loop to iterate over MNIST images for some epochs, optimizing the circuit parameters by minimizing the average  negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f28e9c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:21:45.586683Z",
     "start_time": "2024-10-09T14:18:58.426109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: Average NLL: 2501.196\n",
      "Step 400: Average NLL: 946.254\n",
      "Step 600: Average NLL: 821.438\n",
      "Step 800: Average NLL: 783.204\n",
      "Step 1000: Average NLL: 763.972\n",
      "Step 1200: Average NLL: 752.012\n",
      "Step 1400: Average NLL: 743.246\n",
      "Step 1600: Average NLL: 736.359\n",
      "Step 1800: Average NLL: 732.234\n",
      "Step 2000: Average NLL: 727.142\n",
      "Step 2200: Average NLL: 723.434\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "running_samples = 0\n",
    "\n",
    "# Move the circuit to chosen device\n",
    "circuit = circuit.to(device)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        # The circuit expects an input of shape (batch_dim, num_variables)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch, by evaluating the circuit\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # We take the negated average log-likelihood as loss\n",
    "        loss = -torch.mean(log_likelihoods)\n",
    "        loss.backward()\n",
    "        # Update the parameters of the circuits, as any other model in PyTorch\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        running_samples += len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 200 == 0:\n",
    "            average_nll = running_loss / running_samples\n",
    "            print(f\"Step {step_idx}: Average NLL: {average_nll:.3f}\")\n",
    "            running_loss = 0.0\n",
    "            running_samples = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efae6b",
   "metadata": {},
   "source": [
    "Then, we can evaluate our probabilistic circuit on test data by computing the average log-likelihood and bits per dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e66bd8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T14:21:48.557821Z",
     "start_time": "2024-10-09T14:21:45.715485Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m test_lls \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, _ \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# The circuit expects an input of shape (batch_dim, num_channels, num_variables),\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# so we unsqueeze a dimension for the channel.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Compute the log-likelihoods of the batch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     log_likelihoods \u001b[38;5;241m=\u001b[39m circuit(batch)\n",
      "File \u001b[0;32m~/Workspace/libraries/cirkit/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_lls = 0.0\n",
    "\n",
    "    for batch, _ in test_dataloader:\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Compute the log-likelihoods of the batch\n",
    "        log_likelihoods = circuit(batch)\n",
    "\n",
    "        # Accumulate the log-likelihoods\n",
    "        test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "    # Compute average test log-likelihood and bits per dimension\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (28 * 28 * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d631c2e4",
   "metadata": {},
   "source": [
    "And, finally, we can use the circuit to sample more images as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc914f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cirkit.backend.torch.queries import SamplingQuery\n",
    "\n",
    "num_samples = 1\n",
    "query = SamplingQuery(circuit)\n",
    "\n",
    "samples, _ = query(num_samples=num_samples)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0e15183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7c13d0498a00>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(samples[0].view((28, 28)).numpy(), cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953eb652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
