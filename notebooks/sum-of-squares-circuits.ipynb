{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7369e801-4318-4dee-b77c-5564cc99a6c4",
   "metadata": {},
   "source": [
    "# Building and Learning Sum of Squares Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98126c-27da-4e2b-aff8-b2e966a495ed",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "By the end of this notebook, you will know how to compose symbolic **circuit operators** as to build and learn a Probabilistic Circuit (PC). In particular, you will know how to build and learn Sum-of-Squares (SOS) circuits for distribution estimation tasks, as introduced in the paper [Sum of Squares Circuits](https://arxiv.org/abs/2408.11778). We start by introducing complex squared circuits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c0e47-0c31-47ed-8893-b0a29d797b30",
   "metadata": {},
   "source": [
    "## Complex Squared Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08609db7-439a-48fa-9969-a4b2813c35bc",
   "metadata": {},
   "source": [
    "PCs are typically learned by assuming their parameters to be non-negative, i.e., they are _monotonic_. For example, the PC learned in the notebook [learning-a-circuit.ipynb](learning-a-circuit.ipynb) is monotonic, as it consists of input layers encoding Categorical likelihoods and the parameters are obtained by applying a softmax activation. To build a more expressive distribution estimator, one can instead use a circuit with complex parameters, i.e., a complex circuit.\n",
    "\n",
    "Similarly to the [learning-a-circuit.ipynb](learning-a-circuit.ipynb) notebook, we aim at building a circuit that estimates the probability distribution of MNIST images. For this reason, we will construct a complex circuit using the ```cirkit_templates.image_data``` utility, as shown in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667f60ee-5f58-4146-93cf-7675d222bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates import circuit_templates\n",
    "from cirkit.symbolic.circuit import Circuit\n",
    "\n",
    "def build_symbolic_complex_circuit(region_graph: str) -> Circuit:\n",
    "    return circuit_templates.image_data(\n",
    "        (1, 28, 28),                 # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "        region_graph=region_graph,\n",
    "        # ----------- Input layers hyperparameters ----------- #\n",
    "        input_layer='embedding',     # Use Embedding maps for the pixel values (0-255) as input layers\n",
    "        num_input_units=32,          # Each input layer consists of 32 input units that output Embedding entries\n",
    "        input_params={               # Set how to parameterize the input layers parameters\n",
    "            # In this case we parameterize the 'weight' parameter of Embedding layers,\n",
    "            # by choosing them to be complex-valued whose real and imaginary part are sampled uniformly in [0, 1)\n",
    "            'weight': circuit_templates.Parameterization(dtype='complex', initialization='uniform'),\n",
    "        },\n",
    "        # -------- Sum-product layers hyperparameters -------- #\n",
    "        sum_product_layer='cp-t',    # Use CP-T sum-product layers, i.e., alternate hadamard product layers and dense layers\n",
    "        num_sum_units=32,            # Each dense sum layer consists of 32 sum units\n",
    "        # Set how to parameterize the sum layers parameters\n",
    "        # We paramterize them to be complex-valued whose real and imaginary part are sampled uniformly in [0, 1)\n",
    "        sum_weight_param=circuit_templates.Parameterization(dtype='complex', initialization='uniform')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae740c-0939-4b8e-8437-1d8e82df6f20",
   "metadata": {},
   "source": [
    "In the above, we choose input layers encoding complex embeddings, i.e., each input unit maps a pixel value in $\\{0,1,\\ldots,255\\}$ to the corresponding entry of an embedding in $\\mathbb{C}^{256}$. In addition, we make use of CP-T as sum-product layers, where sum layers are parameterized with complex weights. For more details about this and other layers, see the [region-graph-and-parameterisations.ipynb](region-graph-and-parameterisations.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a296e080-6ec8-4fcc-99c6-3d9662813d4e",
   "metadata": {},
   "source": [
    "To encode a probability distribution, we must at least encode a non-negative real function. To do so with a complex circuit, we take the modulus square of its output. Formally, let $c$ be a complex circuit over variables $\\mathbf{X}$, i.e., $c(\\mathbf{X})\\in\\mathbb{C}$, we can encode a probability distribution $p(\\mathbf{X})$ such that $p(\\mathbf{X})=Z^{-1} |c(\\mathbf{X})|^2 = Z^{-1} c(\\mathbf{X}) c(\\mathbf{X})^*$, where $(\\ \\cdot\\ )^*$ denotes the complex conjugation operation and $Z = \\int_{\\mathrm{dom}(\\mathbf{X})} |c(\\mathbf{x})|^2 \\mathrm{d}\\mathbf{x}$ denotes the partition function. Equivalently, we can write $p(\\mathbf{X})$ as proportional to the **sum of two squares**, i.e., $p(\\mathbf{X}) \\propto \\Re(c(\\mathbf{X}))^2 + \\Im(c(\\mathbf{X}))^2$, where $\\Re,\\Im$ denote real and imaginary part, respectively, thus guaranteeing it is non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfbb3c-3bcf-4499-9953-aa67897c63a2",
   "metadata": {},
   "source": [
    "## Composing Circuit Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104fd1c-522c-46c2-bb75-0d57c1aded2a",
   "metadata": {},
   "source": [
    "To enable the exact and efficient computation of probabilities, we need to renormalize $p$, i.e., compute the renormalization constant $Z$. To do so, we can use the **circuit operators** in the ```cirkit.symbolic.functional``` module as to automatically construct the circuit computing $Z$. All we need is to _compose the operators_ as to encode the formula $Z = \\int_{\\mathrm{dom}(\\mathbf{X})} |c(\\mathbf{x})|^2 \\mathrm{d}\\mathbf{x}$ as yet another circuit.\n",
    "\n",
    "More specifically, each of the operators we will use has **pre-conditions** on the structural properties of the input circuits, and **post-conditions** on the properties and semantics of the output circuit:\n",
    "- ```c' = multiply(c1, c2)```:\n",
    "  - Pre-condition: ```c1``` and ```c2``` are _compatible_, i.e., they share the same partitionings of variables at the products.\n",
    "  - Post-condtion: ```c'``` is _smooth_ and _decomposable_ and encodes the product of ```c1``` and ```c2```.\n",
    "- ```c' = conjugate(c)```:\n",
    "  - Pre-condition: ```c``` is a circuit with possibly complex parameters.\n",
    "  - Post-condition: ```c'``` is a circuit of the same structure of ```c``` and computing the complex conjugation of ```c```.\n",
    "- ```c' = integrate(c)```:\n",
    "  - Pre-condition: ```c``` is a _smooth_ and _decomposable_ circuit.\n",
    "  - Post-condition: ```c'``` is a circuit exactly encoding the integral of ```c``` over the whole variables domain.\n",
    "\n",
    "In order to satisfy these pre-conditions, we construct a complex circuit from a region graph that is structured-decomposable. This will yield a circuit that is compatible with itself, and therefore we can apply the ```multiply``` operator as to square it. Then, the circuit resulting from the multiply operator is smooth and decomposable and therefore it satisfies the pre-conditions of the ```integrate``` operator.\n",
    "\n",
    "We build the symbolic circuit below and show its structural properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0358fb3-989a-4aff-8ab0-a013684eaf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structural properties:\n",
      "  - Smoothness: True\n",
      "  - Decomposability: True\n",
      "  - Structured-decomposability: True\n"
     ]
    }
   ],
   "source": [
    "# Build a symbolic complex circuit by overparameterizing a Quad-Tree (4) region graph, which is structured-decomposable\n",
    "symbolic_circuit = build_symbolic_complex_circuit('quad-tree-4')\n",
    "\n",
    "# Print which structural properties the circuit satisfies\n",
    "print(f'Structural properties:')\n",
    "print(f'  - Smoothness: {symbolic_circuit.is_smooth}')\n",
    "print(f'  - Decomposability: {symbolic_circuit.is_decomposable}')\n",
    "print(f'  - Structured-decomposability: {symbolic_circuit.is_structured_decomposable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4eec54-961f-449f-9562-b6183f916498",
   "metadata": {},
   "source": [
    "Next, we compose the circuit operators mentioned above as to construct the circuit computing $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3222cff6-4423-4c30-8964-6685be991798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cirkit.symbolic.functional as SF\n",
    "\n",
    "# Construct the circuit computing |c(X)|^2 = c(X) c(X)^*\n",
    "symbolic_squared_circuit = SF.multiply(symbolic_circuit, SF.conjugate(symbolic_circuit))\n",
    "\n",
    "# Construct the circuit computing Z, i.e., the integral of |c(X)|^2 over the complete domain of X\n",
    "symbolic_circuit_partition_func = SF.integrate(symbolic_squared_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392528b4-76cc-42c2-b743-3d3de369e673",
   "metadata": {},
   "source": [
    "### Compiling and Learning Complex Squared Circuits\n",
    "\n",
    "Since we want to estimate the distribution of MNIST images, here we learn complex squared circuits by maximizing the log-likelihoods of observed images. Formally, given a complex circuit $c$, we can write the log-likelihood of a data point $\\mathbf{x}$ modeled by the complex squared circuit as $\\log p(\\mathbf{x}) = -\\log Z + 2 \\log |c(\\mathbf{x})|$. Therefore, we need to compile two circuits for this purpose: (1) the circuit $c$, and (2) the circuit computing $Z$.\n",
    "\n",
    "We choose PyTorch as the compilation backend, and set random seeds and the device below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fffd27f7-b18d-4c47-853f-15e62cac7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf6fd2-af42-4fdb-a52f-5dbf7e0fc70e",
   "metadata": {},
   "source": [
    "To compile the circuits, we instantiate a ```PipelineContext``` object and refer the reader to the [compilation-options.ipynb](compilation-options.ipynb) notebook for a tutorial on compiling circuits and on the meaning of the different flags. Here, one important flag is the evaluation semiring. That is, to ensure numerical stability, we evaluate circuits by computing sum and products as they were operations of a semiring where the addition is the LogSumExp and the multiplication is the addition. More specifically, since our complex circuit can have negative real or complex parameter, we choose a generalization of the mentioned semiring over the complex plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f46dc6a2-bf46-49a8-9dc9-d9b3c12411cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import PipelineContext, compile\n",
    "\n",
    "# Instantiate the pipeline context\n",
    "ctx = PipelineContext(\n",
    "    backend='torch',  # Choose PyTorch as compilation backend\n",
    "    # ---- Use the evaluation semiring (C, +, x), where + is the numerically stable LogSumExp and x is the sum ---- #\n",
    "    semiring='complex-lse-sum',\n",
    "    # ------------------------------------------------------------------------------------------------------------- #\n",
    "    fold=True,     # Fold the circuit to better exploit GPU parallelism\n",
    "    optimize=True  # Optimize the layers of the circuit\n",
    ")\n",
    "\n",
    "with ctx:  # Compile the circuits computing log |c(X)| and log |Z|\n",
    "    circuit = compile(symbolic_circuit)\n",
    "    circuit_partition_func = compile(symbolic_circuit_partition_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd0103-80a0-4626-9e14-e994eb115521",
   "metadata": {},
   "source": [
    "In the above code, since we have chosen the ```complex-lse-sum``` semiring, then ```circuit``` is the complex circuit computing $\\log |c(\\mathbf{x})|$, while ```circuit_partition_func``` is the circuit computing $\\log Z$, and both are PyTorch modules.\n",
    "\n",
    "Next, we load the MNIST dataset using ```torchvision```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0dfff6-a526-4a80-a5b8-6f89e911deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Load the MNIST data set and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Flatten the images and set pixel values in the [0-255] range\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "\n",
    "# Initialize a torch optimizer of your choice,\n",
    "#  e.g., Adam, by passing the parameters of the circuit\n",
    "optimizer = optim.Adam(circuit.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24176b6e-a495-4ce3-b627-d893507e2c35",
   "metadata": {},
   "source": [
    "In the following training loop, we move the circuit parameters to the chosen device, and then learn the parameters of the complex squared circuit by minimizing the negative log-likelihood computed on MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02c0673-a16f-4d7f-b7a0-7f8a6507fac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300: Average NLL: 1277.903\n",
      "Step 600: Average NLL: 759.341\n",
      "Step 900: Average NLL: 711.138\n",
      "Step 1200: Average NLL: 683.216\n",
      "Step 1500: Average NLL: 668.198\n",
      "Step 1800: Average NLL: 659.092\n",
      "Step 2100: Average NLL: 653.012\n",
      "Step 2400: Average NLL: 644.388\n",
      "Step 2700: Average NLL: 641.965\n",
      "Step 3000: Average NLL: 640.378\n",
      "Step 3300: Average NLL: 636.618\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "# Move the circuit to chosen device\n",
    "circuit = circuit.to(device)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # -------- Computation of the negative log-likelihoods loss -------- #\n",
    "        # Compute the logarithm of the squared scores of the batch, by evaluating the circuit\n",
    "        log_scores = circuit(batch)                 # log |c(x)|\n",
    "        log_squared_scores = 2.0 * log_scores.real  # 2 * log |c(x)|, i.e., equivalent to log |c(x)|^2\n",
    "        # Compute the log-partition function\n",
    "        log_partition_func = circuit_partition_func().real  # log Z\n",
    "        # Compute the log-likelihoods, log p(x) = 2 * log |c(X)| - log Z\n",
    "        log_likelihoods = log_squared_scores - log_partition_func\n",
    "        # We take the negated average log-likelihood as loss\n",
    "        loss = -torch.mean(log_likelihoods)\n",
    "        # ------------------------------------------------------------------ #\n",
    "\n",
    "        # Update the parameters of the circuits, as any other model in PyTorch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 300 == 0:\n",
    "            print(f\"Step {step_idx}: Average NLL: {running_loss / (300 * len(batch)):.3f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916638d9-6176-487d-8ac9-b048c81680a9",
   "metadata": {},
   "source": [
    "Next, we evaluate the model on the test MNIST images, and show the bits-per-dimension metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5148696f-7721-4cae-8027-7faa0dc33515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: -680.237\n",
      "Bits per dimension: 1.252\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # -------- Compute the log-partition function -------- #\n",
    "    # Note that we need to do it just one, since we are not updating the parameters here\n",
    "    log_partition_func = circuit_partition_func().real\n",
    "    # ---------------------------------------------------- #\n",
    "\n",
    "    test_lls = 0.0\n",
    "    for batch, _ in test_dataloader:\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # -------- Compute the log-likelihoods of hte unseen samples -------- #\n",
    "        # Compute the logarithm of the squared scores of the batch, by evaluating the circuit\n",
    "        log_scores = circuit(batch)\n",
    "        log_squared_scores = 2.0 * log_scores.real\n",
    "        # Compute the log-likelihoods\n",
    "        log_likelihoods = log_squared_scores - log_partition_func\n",
    "        # ------------------------------------------------------------------- #\n",
    "\n",
    "        test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "    # Compute average test log-likelihood and bits per dimension\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (28 * 28 * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa4b21-69c5-4293-80b1-a2ca66e0e9e1",
   "metadata": {},
   "source": [
    "## Learning a Sum of Exponentially many Squared Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0433a0-bded-4481-bc2a-b02757154177",
   "metadata": {},
   "source": [
    "As we also observed above, the complex squared circuit we have built encodes a probability distribution that is the sum of two squares (real and imaginary part of the complex circuit). In the paper [Sum of Squares Circuits](https://arxiv.org/abs/2408.11778), a sum of exponentially many squared circuits is modelled, which can be more expressive than both a single squared circuit with real parameters and a structured-decomposable circuit with positive parameters only. In this section, we construct and learn such a model using cirkit.\n",
    "\n",
    "Given a complex circuit $c_2$ like the one we have previously built, we construct a monotonic PC $c_1$ that has the same structure of $c_2$. By sharing the same structure, we can model the distribution $p(\\mathbf{X})$ as proportional to the product between $c_1$ and the modulus squaring of $c_2$, i.e., $p(\\mathbf{X}) = \\frac{1}{Z} c_1(\\mathbf{X}) |c_2(\\mathbf{X})|^2$, where $Z = \\int_{\\mathrm{dom}(\\mathbf{X})} c_1(\\mathbf{x}) |c_2(\\mathbf{x})|^2 \\mathrm{d}\\mathbf{x}$ is the partition function. Since $c_1$ implicitly encodes a mixture model of an exponential number of components w.r.t. its circuit depth, the product of $c_1(\\mathbf{X})$ and $|c_2(\\mathbf{X})|^2$ results in a mixture model of an exponentialy number of squared circuits that share parameters. See the Appendix C.3 of the paper [Sum of Squares Circuits](https://arxiv.org/abs/2408.11778) for more details.\n",
    "\n",
    "Here, we start by constructing the monotonic and the complex circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be8c730e-933c-4dd6-9cf9-f529dcf9a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_symbolic_monotonic_circuit(region_graph: str) -> Circuit:\n",
    "    return circuit_templates.image_data(\n",
    "        (1, 28, 28),                 # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "        region_graph=region_graph,\n",
    "        # ----------- Input layers hyperparameters ----------- #\n",
    "        input_layer='embedding',     # Use Embedding maps for the pixel values (0-255) as input layers\n",
    "        num_input_units=4,           # Each input layer consists of 4 input units that output Embedding entries\n",
    "        input_params={               # Set how to parameterize the input layers parameters\n",
    "            # In this case we parameterize the 'weight' parameter of Embedding layers,\n",
    "            # by choosing them to be paramerized with a softmax, and initialized by sampling from a standard normal distribution\n",
    "            'weight': circuit_templates.Parameterization(activation='softmax', initialization='normal'),\n",
    "        },\n",
    "        # -------- Sum-product layers hyperparameters -------- #\n",
    "        sum_product_layer='cp-t',    # Use CP-T sum-product layers, i.e., alternate hadamard product layers and dense layers\n",
    "        num_sum_units=4,             # Each dense sum layer consists of 4 sum units\n",
    "        # Set how to parameterize the sum layers parameters\n",
    "        # We paramterize them with a softmax, and initialize them by sampling from a standard normal distribution\n",
    "        sum_weight_param=circuit_templates.Parameterization(activation='softmax', initialization='normal')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c7b8f-a43f-4fd1-a03e-a7e115942ae9",
   "metadata": {},
   "source": [
    "The function above is very similar to the function we used to construct a symbolic complex circuit. The only difference is that we parameterize the embeddings and the sum weights using a softmax activation function, thus guaranteeig the non-negativity of the parameters, and therefore the outputs of $c_1$.\n",
    "\n",
    "By using the same region graph, we construct the symbolic complex and monotonics circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e66e5094-cf4c-445d-9cb7-a1b2b935289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a symbolic monotonic circuit, i..e., c_1, by overparameterizing a Quad-Tree (4) region graph\n",
    "symbolic_monotonic_circuit = build_symbolic_monotonic_circuit('quad-tree-4')\n",
    "\n",
    "# Build a symbolic complex circuit, i.e., c_2, by overparameterizing the same region graph\n",
    "symbolic_complex_circuit = build_symbolic_complex_circuit('quad-tree-4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f99834-9b1c-4408-b36f-c0b1e9544e53",
   "metadata": {},
   "source": [
    "Since we used the same region graph, the two circuits will be _compatible_, thus satisfying the pre-conditions of the ```multiply``` operator.\n",
    "\n",
    "In the following code snipped, we construct the symbolic circuit computing the partition function  of our model, i.e., $Z = \\int_{\\mathrm{dom}(\\mathbf{X})} c_1(\\mathbf{x}) |c_2(\\mathbf{x})|^2 \\mathrm{d}\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "150b9612-9341-41bc-bd44-fa7c9be729c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the circuit computing c_1(X) |c_2(X)|^2 = c_1(X) c_2(X) c_2(X)^*\n",
    "symbolic_expsos_circuit = SF.multiply(\n",
    "    symbolic_monotonic_circuit,\n",
    "    SF.multiply(symbolic_complex_circuit, SF.conjugate(symbolic_complex_circuit))\n",
    ")\n",
    "\n",
    "# Construct the circuit computing Z, i.e., the integral of c_1(X) |c_2(X)|^2 over the complete domain of X\n",
    "symbolic_circuit_partition_func = SF.integrate(symbolic_expsos_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143823e-5d2f-4e09-b107-ad70f89b4263",
   "metadata": {},
   "source": [
    "As done in the previous section, we compile the circuits we need during learning. We observe we can decompose the log-likelihood $\\log p(\\mathbf{x})$ of a data point $\\mathbf{x}$ as\n",
    "$$\\log p(\\mathbf{x}) = -\\log Z + \\log c_1(\\mathbf{x}) + 2 \\log |c_2(\\mathbf{x})|,$$\n",
    "thus requiring us to compile three circuits: (1) the monotonic circuit $c_1$, (2) the complex circuit $c_2$, and (3) the circuit computing $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "454efc3f-df9c-4b10-bee6-f496829b4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free-up some memory\n",
    "del circuit, circuit_partition_func, ctx\n",
    "\n",
    "# Instantiate the pipeline context\n",
    "ctx = PipelineContext(\n",
    "    backend='torch',  # Choose PyTorch as compilation backend\n",
    "    semiring='complex-lse-sum',\n",
    "    fold=True,     # Fold the circuit to better exploit GPU parallelism\n",
    "    optimize=True  # Optimize the layers of the circuit\n",
    ")\n",
    "\n",
    "with ctx:  # Compile the circuits computing log c_1(X), log |c_2(X)|, and log |Z|\n",
    "    monotonic_circuit = compile(symbolic_monotonic_circuit)\n",
    "    complex_circuit = compile(symbolic_complex_circuit)\n",
    "    circuit_partition_func = compile(symbolic_circuit_partition_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddab9ad-4d72-4c8a-9657-dbc552c311cb",
   "metadata": {},
   "source": [
    "In the following, we use Adam as optimizer in PyTorch and optimize the learnable parameters of both the monotonic and complex circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7775ac5b-b9c4-48d4-baae-fa8da5770530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Initialize a torch optimizer of your choice,\n",
    "#  e.g., Adam, by passing the parameters of the circuits\n",
    "optimizer = optim.Adam(itertools.chain(monotonic_circuit.parameters(), complex_circuit.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca1d9e-3fa3-4c49-96dc-4267f6fac04c",
   "metadata": {},
   "source": [
    "As done for the complex squared circuit above, we optimize the parameters by minimizing the negative log-likelihood computed on MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b05a77-4063-4c5d-a725-376ff35d7a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300: Average NLL: 1192.256\n",
      "Step 600: Average NLL: 731.775\n",
      "Step 900: Average NLL: 689.649\n",
      "Step 1200: Average NLL: 665.281\n",
      "Step 1500: Average NLL: 651.504\n",
      "Step 1800: Average NLL: 642.295\n",
      "Step 2100: Average NLL: 636.493\n",
      "Step 2400: Average NLL: 628.150\n",
      "Step 2700: Average NLL: 626.221\n",
      "Step 3000: Average NLL: 623.579\n",
      "Step 3300: Average NLL: 620.558\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "# Move the circuits to chosen device\n",
    "monotonic_circuit = monotonic_circuit.to(device)\n",
    "complex_circuit = complex_circuit.to(device)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # -------- Computation of the negative log-likelihoods loss -------- #\n",
    "        # Compute the logarithm of the scores of the batch, by evaluating the circuits\n",
    "        log_monotonic_scores = monotonic_circuit(batch).real    # log c_+(x)\n",
    "        log_squared_scores = 2.0 * complex_circuit(batch).real  # 2 * log |c(x)|\n",
    "        # Compute the log-partition function\n",
    "        log_partition_func = circuit_partition_func().real  # log Z\n",
    "        # Compute the log-likelihoods, log p(x) = log c_+(x) + 2 * log |c(X)| - log Z\n",
    "        log_likelihoods = log_monotonic_scores + log_squared_scores - log_partition_func\n",
    "        # We take the negated average log-likelihood as loss\n",
    "        loss = -torch.mean(log_likelihoods)\n",
    "        # ------------------------------------------------------------------ #\n",
    "\n",
    "        # Update the parameters of the circuits, as any other model in PyTorch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 300 == 0:\n",
    "            print(f\"Step {step_idx}: Average NLL: {running_loss / (300 * len(batch)):.3f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69680885-e0bb-4fc0-8079-1a3d905160d3",
   "metadata": {},
   "source": [
    "Then, we test the model by computing the bits-per-dimension on unseen MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6edb6c7e-a07e-4a14-954c-f2593e2772cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: -667.903\n",
      "Bits per dimension: 1.229\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # -------- Compute the log-partition function -------- #\n",
    "    # Note that we need to do it just one, since we are not updating the parameters here\n",
    "    log_partition_func = circuit_partition_func().real\n",
    "    # ---------------------------------------------------- #\n",
    "\n",
    "    test_lls = 0.0\n",
    "    for batch, _ in test_dataloader:\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # -------- Compute the log-likelihoods of hte unseen samples -------- #\n",
    "        # Compute the logarithm of the cores of the batch, by evaluating the circuits\n",
    "        log_monotonic_scores = monotonic_circuit(batch).real    # log c_+(x)\n",
    "        log_squared_scores = 2.0 * complex_circuit(batch).real  # 2 * log |c(x)|\n",
    "        # Compute the log-likelihoods\n",
    "        log_likelihoods = log_monotonic_scores + log_squared_scores - log_partition_func\n",
    "        # ------------------------------------------------------------------- #\n",
    "\n",
    "        test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "    # Compute average test log-likelihood and bits per dimension\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (28 * 28 * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af142a-e055-4915-9a4f-3512660d4790",
   "metadata": {},
   "source": [
    "Note that we achieved a reduction in terms of bits-per-dimension when compared to the complex squared circuit alone. In particular, this reduction has been achieved with a small increase in the total number of parameters given by the monotonic circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62c9a36b-de54-4bdc-8c6f-218473665e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monotonic circuit - Num. of parameters: 807044\n",
      "Complex circuit - Num. of parameters: 13385792\n"
     ]
    }
   ],
   "source": [
    "num_monotonic_params = sum(t.numel() for t in monotonic_circuit.parameters() if t.requires_grad)\n",
    "num_complex_params = sum(2 * t.numel() for t in complex_circuit.parameters() if t.requires_grad)\n",
    "print(f\"Monotonic circuit - Num. of parameters: {num_monotonic_params}\")\n",
    "print(f\"Complex circuit - Num. of parameters: {num_complex_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
