{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7369e801-4318-4dee-b77c-5564cc99a6c4",
   "metadata": {},
   "source": [
    "# Building and Learning Sum of Squares Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98126c-27da-4e2b-aff8-b2e966a495ed",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "By the end of this notebook, you will know how to compose symbolic **circuit operators** as to build and learn a Probabilistic Circuit (PC). In particular, you will know how to build and learn Sum-of-Squares (SOS) circuits for distribution estimation tasks, as introduced in the paper [Sum of Squares Circuits](https://arxiv.org/abs/2408.11778). We start by introducing complex squared circuits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c0e47-0c31-47ed-8893-b0a29d797b30",
   "metadata": {},
   "source": [
    "## Complex Squared Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08609db7-439a-48fa-9969-a4b2813c35bc",
   "metadata": {},
   "source": [
    "PCs are typically learned by assuming their parameters to be non-negative, i.e., they are _monotonic_. For example, the PC learned in the notebook [learning-a-circuit.ipynb](learning-a-circuit.ipynb) is monotonic, as it consists of input layers encoding Categorical likelihoods and the parameters are obtained by applying a softmax activation. To build a more expressive distribution estimator, one can instead use a circuit with complex parameters, i.e., a complex circuit.\n",
    "\n",
    "Similarly to the [learning-a-circuit.ipynb](learning-a-circuit.ipynb) notebook, we aim at building a circuit that estimates the probability distribution of MNIST images. For this reason, we will construct a complex circuit using the ```cirkit_templates.image_data``` utility, as shown in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667f60ee-5f58-4146-93cf-7675d222bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates import circuit_templates\n",
    "from cirkit.symbolic.circuit import Circuit\n",
    "\n",
    "def build_symbolic_complex_circuit(region_graph: str) -> Circuit:\n",
    "    return circuit_templates.image_data(\n",
    "        (1, 28, 28),                 # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "        region_graph=region_graph,\n",
    "        # ----------- Input layers hyperparameters ----------- #\n",
    "        input_layer='embedding',     # Use Embedding maps for the pixel values (0-255) as input layers\n",
    "        num_input_units=32,          # Each input layer consists of 32 input units that output Embedding entries\n",
    "        input_params={               # Set how to parameterize the input layers parameters\n",
    "            # In this case we parameterize the 'weight' parameter of Embedding layers,\n",
    "            # by choosing them to be complex-valued whose real and imaginary part are sampled uniformly in [0, 1)\n",
    "            'weight': circuit_templates.Parameterization(dtype='complex', initialization='uniform'),\n",
    "        },\n",
    "        # -------- Sum-product layers hyperparameters -------- #\n",
    "        sum_product_layer='cp-t',    # Use CP-T sum-product layers, i.e., alternate hadamard product layers and dense layers\n",
    "        num_sum_units=32,            # Each dense sum layer consists of 32 sum units\n",
    "        # Set how to parameterize the sum layers parameters\n",
    "        # We paramterize them to be complex-valued whose real and imaginary part are sampled uniformly in [0, 1)\n",
    "        sum_weight_param=circuit_templates.Parameterization(dtype='complex', initialization='uniform')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae740c-0939-4b8e-8437-1d8e82df6f20",
   "metadata": {},
   "source": [
    "In the above, we choose input layers encoding complex embeddings, i.e., each input unit maps a pixel value in $\\{0,1,\\ldots,255\\}$ to the corresponding entry of an embedding in $\\mathbb{C}^{256}$. In addition, we make use of CP-T as sum-product layers, where sum layers are parameterized with complex weights. For more details about this and other layers, see the [region-graph-and-parameterisations.ipynb](region-graph-and-parameterisations.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a296e080-6ec8-4fcc-99c6-3d9662813d4e",
   "metadata": {},
   "source": [
    "To encode a probability distribution, we must at least encode a non-negative real function. To do so with a complex circuit, we take the modulus square of its output. Formally, let $c$ be a complex circuit over variables $\\mathbf{X}$, i.e., $c(\\mathbf{X})\\in\\mathbb{C}$, we can encode a probability distribution $p(\\mathbf{X})$ such that $p(\\mathbf{X})=Z^{-1} |c(\\mathbf{X})|^2 = Z^{-1} c(\\mathbf{X}) c(\\mathbf{X})^*$, where $(\\ \\cdot\\ )^*$ denotes the complex conjugation operation and $Z = \\int_{\\mathrm{dom}(\\mathbf{X})} |c(\\mathbf{x})|^2 \\mathrm{d}\\mathbf{x}$ denotes the partition function. Equivalently, we can write $p(\\mathbf{X})$ as proportional to the **sum of two squares**, i.e., $p(\\mathbf{X}) \\propto \\Re(c(\\mathbf{X}))^2 + \\Im(c(\\mathbf{X}))^2$, where $\\Re,\\Im$ denote real and imaginary part, respectively, thus guaranteeing it is non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfbb3c-3bcf-4499-9953-aa67897c63a2",
   "metadata": {},
   "source": [
    "## Composing Circuit Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104fd1c-522c-46c2-bb75-0d57c1aded2a",
   "metadata": {},
   "source": [
    "To enable the exact and efficient computation of probabilities, we need to renormalize $p$, i.e., compute the renormalization constant $Z$. To do so, we can use the **circuit operators** in the ```cirkit.symbolic.functional``` module as to automatically construct the circuit computing $Z$. All we need is to _compose the operators_ as to encode the formula $Z = \\int_{\\mathrm{dom}(\\mathbf{X})} |c(\\mathbf{x})|^2 \\mathrm{d}\\mathbf{x}$ as yet another circuit.\n",
    "\n",
    "More specifically, each of the operators we will use has **pre-conditions** on the structural properties of the input circuits, and **post-conditions** on the properties and semantics of the output circuit:\n",
    "- ```c' = multiply(c1, c2)```:\n",
    "  - Pre-condition: ```c1``` and ```c2``` are _compatible_, i.e., they share the same partitionings of variables at the products.\n",
    "  - Post-condtion: ```c'``` is _smooth_ and _decomposable_ and encodes the product of ```c1``` and ```c2```.\n",
    "- ```c' = conjugate(c)```:\n",
    "  - Pre-condition: ```c``` is a circuit with possibly complex parameters.\n",
    "  - Post-condition: ```c'``` is a circuit of the same structure of ```c``` and computing the complex conjugation of ```c```.\n",
    "- ```c' = integrate(c)```:\n",
    "  - Pre-condition: ```c``` is a _smooth_ and _decomposable_ circuit.\n",
    "  - Post-condition: ```c'``` is a circuit exactly encoding the integral of ```c``` over the whole variables domain.\n",
    "\n",
    "In order to satisfy these pre-conditions, we construct a complex circuit from a region graph that is structured-decomposable. This will yield a circuit that is compatible with itself, and therefore we apply the ```multiply``` operator as to square it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0358fb3-989a-4aff-8ab0-a013684eaf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structural properties:\n",
      "  - Smoothness: True\n",
      "  - Decomposability: True\n",
      "  - Structured-decomposability: True\n"
     ]
    }
   ],
   "source": [
    "# Build a symbolic complex circuit by overparameterizing a Quad-Tree (4) region graph, which is structured-decomposable\n",
    "symbolic_circuit = build_symbolic_complex_circuit('quad-tree-4')\n",
    "\n",
    "# Print which structural properties the circuit satisfies\n",
    "print(f'Structural properties:')\n",
    "print(f'  - Smoothness: {symbolic_circuit.is_smooth}')\n",
    "print(f'  - Decomposability: {symbolic_circuit.is_decomposable}')\n",
    "print(f'  - Structured-decomposability: {symbolic_circuit.is_structured_decomposable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4eec54-961f-449f-9562-b6183f916498",
   "metadata": {},
   "source": [
    "Next, we compose the circuit operators mentioned above as to construct the circuit computing $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3222cff6-4423-4c30-8964-6685be991798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cirkit.symbolic.functional as SF\n",
    "\n",
    "# Construct the circuit computing |c(X)|^2 = c(X) c(X)^*\n",
    "symbolic_squared_circuit = SF.multiply(symbolic_circuit, SF.conjugate(symbolic_circuit))\n",
    "\n",
    "# Construct the circuit computing Z, i.e., the integral of |c(X)|^2 over the complete domain of X\n",
    "symbolic_circuit_partition_func = SF.integrate(symbolic_squared_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392528b4-76cc-42c2-b743-3d3de369e673",
   "metadata": {},
   "source": [
    "### Compiling and Learning Complex Squared Circuits\n",
    "\n",
    "Since we want to estimate the distribution of MNIST images, here we learn complex squared circuits by maximizing the log-likelihoods of observed images. Formally, given a complex circuit $c$, we can write the log-likelihood of a data point $\\mathbf{x}$ modeled by the complex squared circuit as $\\log p(\\mathbf{x}) = -\\log Z + 2 \\log |c(\\mathbf{x})|$. Therefore, we need to compile two circuits for this purpose: (1) the circuit $c$, and (2) the circuit computing $Z$.\n",
    "\n",
    "We choose PyTorch as the compilation backend, and set random seeds and the device below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fffd27f7-b18d-4c47-853f-15e62cac7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf6fd2-af42-4fdb-a52f-5dbf7e0fc70e",
   "metadata": {},
   "source": [
    "To compile the circuits, we instantiate a ```PipelineContext``` object and refer the reader to the [compilation-options.ipynb](compilation-options.ipynb) notebook for a tutorial on compiling circuits and on the meaning of the different flags. Here, one important flag is the evaluation semiring. That is, to ensure numerical stability, we evaluate circuits by computing sum and products as they were operations of a semiring where the addition is the LogSumExp and the multiplication is the addition. More specifically, since our complex circuit can have negative real or complex parameter, we choose a generalization of the mentioned semiring over the complex plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f46dc6a2-bf46-49a8-9dc9-d9b3c12411cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import PipelineContext, compile\n",
    "\n",
    "# Instantiate the pipeline context\n",
    "ctx = PipelineContext(\n",
    "    backend='torch',  # Choose PyTorch as compilation backend\n",
    "    # ---- Use the evaluation semiring (C, +, x), where + is the numerically stable LogSumExp and x is the sum ---- #\n",
    "    semiring='complex-lse-sum',\n",
    "    # ------------------------------------------------------------------------------------------------------------- #\n",
    "    fold=True,     # Fold the circuit to better exploit GPU parallelism\n",
    "    optimize=True  # Optimize the layers of the circuit\n",
    ")\n",
    "\n",
    "with ctx:  # Compile the circuits computing log |c(X)| and log |Z|\n",
    "    circuit = compile(symbolic_circuit)\n",
    "    circuit_partition_func = compile(symbolic_circuit_partition_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd0103-80a0-4626-9e14-e994eb115521",
   "metadata": {},
   "source": [
    "In the above code, since we have chosen the ```complex-lse-sum``` semiring, then ```circuit``` is the complex circuit computing $\\log |c(\\mathbf{x})|$, while ```circuit_partition_func``` is the circuit computing $\\log Z$, and both are PyTorch modules.\n",
    "\n",
    "Next, we load the MNIST dataset using ```torchvision```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0dfff6-a526-4a80-a5b8-6f89e911deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Load the MNIST data set and data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Flatten the images and set pixel values in the [0-255] range\n",
    "    transforms.Lambda(lambda x: (255 * x.view(-1)).long())\n",
    "])\n",
    "data_train = datasets.MNIST('datasets', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST('datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Instantiate the training and testing data loaders\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=256)\n",
    "test_dataloader = DataLoader(data_test, shuffle=False, batch_size=256)\n",
    "\n",
    "# Initialize a torch optimizer of your choice,\n",
    "#  e.g., Adam, by passing the parameters of the circuit\n",
    "optimizer = optim.Adam(circuit.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24176b6e-a495-4ce3-b627-d893507e2c35",
   "metadata": {},
   "source": [
    "In the following training loop, we move the circuit parameters to the chosen device, and then learn the parameters of the complex squared circuit by minimizing the negative log-likelihood computed on MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02c0673-a16f-4d7f-b7a0-7f8a6507fac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: Average NLL: 2127.237\n",
      "Step 200: Average NLL: 895.312\n",
      "Step 300: Average NLL: 811.160\n",
      "Step 400: Average NLL: 784.650\n",
      "Step 500: Average NLL: 759.404\n",
      "Step 600: Average NLL: 733.968\n",
      "Step 700: Average NLL: 729.767\n",
      "Step 800: Average NLL: 699.855\n",
      "Step 900: Average NLL: 703.793\n",
      "Step 1000: Average NLL: 687.243\n",
      "Step 1100: Average NLL: 684.919\n",
      "Step 1200: Average NLL: 677.487\n",
      "Step 1300: Average NLL: 672.656\n",
      "Step 1400: Average NLL: 674.011\n",
      "Step 1500: Average NLL: 657.926\n",
      "Step 1600: Average NLL: 665.834\n",
      "Step 1700: Average NLL: 654.318\n",
      "Step 1800: Average NLL: 657.123\n",
      "Step 1900: Average NLL: 653.287\n",
      "Step 2000: Average NLL: 650.353\n",
      "Step 2100: Average NLL: 655.398\n",
      "Step 2200: Average NLL: 641.939\n",
      "Step 2300: Average NLL: 648.654\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "# Move the circuit to chosen device\n",
    "circuit = circuit.to(device)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # -------- Computation of the negative log-likelihoods loss -------- #\n",
    "        # Compute the logarithm of the squared scores of the batch, by evaluating the circuit\n",
    "        log_scores = circuit(batch)                 # log |c(x)|\n",
    "        log_squared_scores = 2.0 * log_scores.real  # 2 * log |c(x)|, i.e., equivalent to log |c(x)|^2\n",
    "        # Compute the log-partition function\n",
    "        log_partition_func = circuit_partition_func().real  # log Z\n",
    "        # Compute the log-likelihoods, log p(x) = 2 * log |c(X)| - log Z\n",
    "        log_likelihoods = log_squared_scores - log_partition_func\n",
    "        # We take the negated average log-likelihood as loss\n",
    "        loss = -torch.mean(log_likelihoods)\n",
    "        # ------------------------------------------------------------------ #\n",
    "\n",
    "        # Update the parameters of the circuits, as any other model in PyTorch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 100 == 0:\n",
    "            print(f\"Step {step_idx}: Average NLL: {running_loss / (100 * len(batch)):.3f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916638d9-6176-487d-8ac9-b048c81680a9",
   "metadata": {},
   "source": [
    "Next, we evaluate the model on the test MNIST images, and show the bits-per-dimension metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5148696f-7721-4cae-8027-7faa0dc33515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: -684.520\n",
      "Bits per dimension: 1.260\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # -------- Compute the log-partition function -------- #\n",
    "    # Note that we need to do it just one, since we are not updating the parameters here\n",
    "    log_partition_func = circuit_partition_func().real\n",
    "    # ---------------------------------------------------- #\n",
    "\n",
    "    test_lls = 0.0\n",
    "    for batch, _ in test_dataloader:\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # -------- Compute the log-likelihoods of hte unseen samples -------- #\n",
    "        # Compute the logarithm of the squared scores of the batch, by evaluating the circuit\n",
    "        log_scores = circuit(batch)\n",
    "        log_squared_scores = 2.0 * log_scores.real\n",
    "        # Compute the log-likelihoods\n",
    "        log_likelihoods = log_squared_scores - log_partition_func\n",
    "        # ------------------------------------------------------------------- #\n",
    "\n",
    "        test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "    # Compute average test log-likelihood and bits per dimension\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (28 * 28 * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c3761-31ed-4601-8fda-45416b598c1c",
   "metadata": {},
   "source": [
    "The learned complex squared circuit achieved a lower bits-per-dimension than the monotonic PC learned in the [learning-a-circuit.ipynb](learning-a-circuit.ipynb) notebook, with about the same number of learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa4b21-69c5-4293-80b1-a2ca66e0e9e1",
   "metadata": {},
   "source": [
    "## Learning a Sum of Exponentially many Squared Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0433a0-bded-4481-bc2a-b02757154177",
   "metadata": {},
   "source": [
    "TODO: write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be8c730e-933c-4dd6-9cf9-f529dcf9a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_symbolic_monotonic_circuit(region_graph: str) -> Circuit:\n",
    "    return circuit_templates.image_data(\n",
    "        (1, 28, 28),                 # The shape of MNIST image, i.e., (num_channels, image_height, image_width)\n",
    "        region_graph=region_graph,\n",
    "        # ----------- Input layers hyperparameters ----------- #\n",
    "        input_layer='embedding',     # Use Embedding maps for the pixel values (0-255) as input layers\n",
    "        num_input_units=4,           # Each input layer consists of 4 input units that output Embedding entries\n",
    "        input_params={               # Set how to parameterize the input layers parameters\n",
    "            # In this case we parameterize the 'weight' parameter of Embedding layers,\n",
    "            # by choosing them to be paramerized with a softmax, and initialized by sampling from a standard normal distribution\n",
    "            'weight': circuit_templates.Parameterization(activation='softmax', initialization='normal'),\n",
    "        },\n",
    "        # -------- Sum-product layers hyperparameters -------- #\n",
    "        sum_product_layer='cp-t',    # Use CP-T sum-product layers, i.e., alternate hadamard product layers and dense layers\n",
    "        num_sum_units=4,             # Each dense sum layer consists of 4 sum units\n",
    "        # Set how to parameterize the sum layers parameters\n",
    "        # We paramterize them with a softmax, and initialize them by sampling from a standard normal distribution\n",
    "        sum_weight_param=circuit_templates.Parameterization(activation='softmax', initialization='normal')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c7b8f-a43f-4fd1-a03e-a7e115942ae9",
   "metadata": {},
   "source": [
    "TODO: write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e66e5094-cf4c-445d-9cb7-a1b2b935289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a symbolic complex circuit by overparameterizing a Quad-Tree (4) region graph, which is structured-decomposable\n",
    "symbolic_complex_circuit = build_symbolic_complex_circuit('quad-tree-4')\n",
    "\n",
    "# Build a symbolic monotonic circuit, with the same region graph\n",
    "symbolic_monotonic_circuit = build_symbolic_monotonic_circuit('quad-tree-4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f99834-9b1c-4408-b36f-c0b1e9544e53",
   "metadata": {},
   "source": [
    "TODO: write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "150b9612-9341-41bc-bd44-fa7c9be729c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the circuit computing c_+(X) |c(X)|^2 = c_+(X) c(X) c(X)^*\n",
    "symbolic_expsos_circuit = SF.multiply(\n",
    "    symbolic_monotonic_circuit,\n",
    "    SF.multiply(symbolic_complex_circuit, SF.conjugate(symbolic_complex_circuit))\n",
    ")\n",
    "\n",
    "# Construct the circuit computing Z, i.e., the integral of c_+(X) |c(X)|^2 over the complete domain of X\n",
    "symbolic_circuit_partition_func = SF.integrate(symbolic_expsos_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143823e-5d2f-4e09-b107-ad70f89b4263",
   "metadata": {},
   "source": [
    "TODO: write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "454efc3f-df9c-4b10-bee6-f496829b4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free-up some memory\n",
    "del circuit, circuit_partition_func, ctx\n",
    "\n",
    "# Instantiate the pipeline context\n",
    "ctx = PipelineContext(\n",
    "    backend='torch',  # Choose PyTorch as compilation backend\n",
    "    semiring='complex-lse-sum',\n",
    "    fold=True,     # Fold the circuit to better exploit GPU parallelism\n",
    "    optimize=True  # Optimize the layers of the circuit\n",
    ")\n",
    "\n",
    "with ctx:  # Compile the circuits computing log c_+(X), log |c(X)|, and log |Z|\n",
    "    monotonic_circuit = compile(symbolic_monotonic_circuit)\n",
    "    complex_circuit = compile(symbolic_complex_circuit)\n",
    "    circuit_partition_func = compile(symbolic_circuit_partition_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddab9ad-4d72-4c8a-9657-dbc552c311cb",
   "metadata": {},
   "source": [
    "TODO: write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7775ac5b-b9c4-48d4-baae-fa8da5770530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Initialize a torch optimizer of your choice,\n",
    "#  e.g., Adam, by passing the parameters of the circuits\n",
    "optimizer = optim.Adam(itertools.chain(monotonic_circuit.parameters(), complex_circuit.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b05a77-4063-4c5d-a725-376ff35d7a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: Average NLL: 1970.952\n",
      "Step 200: Average NLL: 835.667\n",
      "Step 300: Average NLL: 773.546\n",
      "Step 400: Average NLL: 754.928\n",
      "Step 500: Average NLL: 731.176\n",
      "Step 600: Average NLL: 710.194\n",
      "Step 700: Average NLL: 707.972\n",
      "Step 800: Average NLL: 680.812\n",
      "Step 900: Average NLL: 683.363\n",
      "Step 1000: Average NLL: 666.073\n",
      "Step 1100: Average NLL: 665.810\n",
      "Step 1200: Average NLL: 658.401\n",
      "Step 1300: Average NLL: 655.889\n",
      "Step 1400: Average NLL: 654.304\n",
      "Step 1500: Average NLL: 639.108\n",
      "Step 1600: Average NLL: 645.786\n",
      "Step 1700: Average NLL: 637.203\n",
      "Step 1800: Average NLL: 637.141\n",
      "Step 1900: Average NLL: 635.162\n",
      "Step 2000: Average NLL: 631.744\n",
      "Step 2100: Average NLL: 635.326\n",
      "Step 2200: Average NLL: 622.690\n",
      "Step 2300: Average NLL: 630.649\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "step_idx = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "# Move the circuits to chosen device\n",
    "monotonic_circuit = monotonic_circuit.to(device)\n",
    "complex_circuit = complex_circuit.to(device)\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (batch, _) in enumerate(train_dataloader):\n",
    "        # The circuit expects an input of shape (batch_dim, num_channels, num_variables),\n",
    "        # so we unsqueeze a dimension for the channel.\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # -------- Computation of the negative log-likelihoods loss -------- #\n",
    "        # Compute the logarithm of the scores of the batch, by evaluating the circuits\n",
    "        log_monotonic_scores = monotonic_circuit(batch).real    # log c_+(x)\n",
    "        log_squared_scores = 2.0 * complex_circuit(batch).real  # 2 * log |c(x)|\n",
    "        # Compute the log-partition function\n",
    "        log_partition_func = circuit_partition_func().real  # log Z\n",
    "        # Compute the log-likelihoods, log p(x) = log c_+(x) + 2 * log |c(X)| - log Z\n",
    "        log_likelihoods = log_monotonic_scores + log_squared_scores - log_partition_func\n",
    "        # We take the negated average log-likelihood as loss\n",
    "        loss = -torch.mean(log_likelihoods)\n",
    "        # ------------------------------------------------------------------ #\n",
    "\n",
    "        # Update the parameters of the circuits, as any other model in PyTorch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.detach() * len(batch)\n",
    "        step_idx += 1\n",
    "        if step_idx % 100 == 0:\n",
    "            print(f\"Step {step_idx}: Average NLL: {running_loss / (100 * len(batch)):.3f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69680885-e0bb-4fc0-8079-1a3d905160d3",
   "metadata": {},
   "source": [
    "TODO: write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6edb6c7e-a07e-4a14-954c-f2593e2772cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test LL: -666.170\n",
      "Bits per dimension: 1.226\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # -------- Compute the log-partition function -------- #\n",
    "    # Note that we need to do it just one, since we are not updating the parameters here\n",
    "    log_partition_func = circuit_partition_func().real\n",
    "    # ---------------------------------------------------- #\n",
    "\n",
    "    test_lls = 0.0\n",
    "    for batch, _ in test_dataloader:\n",
    "        batch = batch.to(device).unsqueeze(dim=1)\n",
    "\n",
    "        # -------- Compute the log-likelihoods of hte unseen samples -------- #\n",
    "        # Compute the logarithm of the cores of the batch, by evaluating the circuits\n",
    "        log_monotonic_scores = monotonic_circuit(batch).real    # log c_+(x)\n",
    "        log_squared_scores = 2.0 * complex_circuit(batch).real  # 2 * log |c(x)|\n",
    "        # Compute the log-likelihoods\n",
    "        log_likelihoods = log_monotonic_scores + log_squared_scores - log_partition_func\n",
    "        # ------------------------------------------------------------------- #\n",
    "\n",
    "        test_lls += log_likelihoods.sum().item()\n",
    "\n",
    "    # Compute average test log-likelihood and bits per dimension\n",
    "    average_ll = test_lls / len(data_test)\n",
    "    bpd = -average_ll / (28 * 28 * np.log(2.0))\n",
    "    print(f\"Average test LL: {average_ll:.3f}\")\n",
    "    print(f\"Bits per dimension: {bpd:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af142a-e055-4915-9a4f-3512660d4790",
   "metadata": {},
   "source": [
    "TODO: write"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
