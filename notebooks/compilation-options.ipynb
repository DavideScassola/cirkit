{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61029f7-31c0-41d3-ad93-ed730b12646e",
   "metadata": {},
   "source": [
    "# Setting Options of the Compilation Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa8c28-cc8b-45bd-9f10-3dd356c74777",
   "metadata": {},
   "source": [
    "TODO: what are we going to see here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e67417-c95d-4df0-a867-94bcb344f64c",
   "metadata": {},
   "source": [
    "TODO: let's start by creating a simple symbolic circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "295597da-8a26-4ec6-9ffc-4b30feacc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.templates import circuit_templates\n",
    "\n",
    "symbolic_circuit = circuit_templates.image_data(\n",
    "    (1, 28, 28),                # The shape of the image, i.e., (num_channels, image_height, image_width)\n",
    "    region_graph='quad-graph',  # Select the structure of the circuit to follow the QuadGraph region graph\n",
    "    input_layer='categorical',  # Use Categorical distributions for the pixel values (0-255) as input layers\n",
    "    num_input_units=4,          # Each input layer consists of 32 Categorical input units\n",
    "    sum_product_layer='tucker', # Use Tucker sum-product layers, i.e., alternate dense sum layers and kronecker product layers\n",
    "    num_sum_units=4,            # Each dense sum layer consists of 32 sum units\n",
    "    sum_weight_param='softmax'  # Parameterize the weights of dense sum layers with 'softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284040e3-6b21-4cd6-b69b-c7135ebe0b40",
   "metadata": {},
   "source": [
    "## The Pipeline Context object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8026fd-3916-4064-960e-39654b07e491",
   "metadata": {},
   "source": [
    "TODO: outline the pipeline context object and explain the semiring as well as the compilation flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "048243c9-1225-483e-a61c-ba3ca1bc4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds and the torch device\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set some seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the torch device to use\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f9e598b6-7ccb-4edc-9ff6-dd88d6c1af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirkit.pipeline import PipelineContext\n",
    "\n",
    "ctx = PipelineContext(\n",
    "    backend='torch',      # Use the PyTorch backend\n",
    "    # Specify the backend compilation flags next\n",
    "    semiring='lse-sum',   # Specify how to evaluate sum and product layers\n",
    "                          # In this case we use the numerically-stable LogSumExp-Sum semiring (R, +, *),\n",
    "                          # where: + is the log-sum-exp operation, and * is the sum operation.\n",
    "    fold=False,           # Disable folding (for now)\n",
    "    optimize=False,       # Disable layer optimizations (for now)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e6f46d-9211-4dd2-b259-1651dff56832",
   "metadata": {},
   "source": [
    "TODO: next, we compile our symbolic circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a086946-4d55-4d70-867e-be2a60ca7fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 818 ms, sys: 6.66 ms, total: 824 ms\n",
      "Wall time: 812 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "circuit = ctx.compile(symbolic_circuit)\n",
    "\n",
    "# Alternative way to compile a circuit using a Pipeline Context:\n",
    "#\n",
    "#from cirkit.pipeline import compile\n",
    "#\n",
    "#with ctx:\n",
    "#    circuit = compile(symbolic_circuit)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d136e99-ecdf-47cf-8f26-aa189ceb8cfd",
   "metadata": {},
   "source": [
    "TODO: explain the output values of a circuit compiled with the 'lse-sum' semiring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa834ad-3a76-4f29-a6a9-446191324ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ca38f44-888a-4133-ad17-68ff0d6797ba",
   "metadata": {},
   "source": [
    "## Optimizing your Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc94870-f097-474a-9bd4-322204b22624",
   "metadata": {},
   "source": [
    "TODO: outline folding and optimization rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bba636-3150-4d6c-bed0-b1505ea4abb0",
   "metadata": {},
   "source": [
    "TODO: let's start by benchmarking the unoptimized circuit we have compiled above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e527a96-4659-44c2-9bea-b161963f1032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 s, sys: 109 ms, total: 2.25 s\n",
      "Wall time: 608 ms\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch = torch.randint(256, size=(256, 1, 784), device=device)\n",
    "circuit(batch)\n",
    "if 'cuda' in str(device):\n",
    "    torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb2226-81a9-4909-bd09-ab8c2d6749db",
   "metadata": {},
   "source": [
    "TODO: Why Would I disable Optimizations? disabling optimizations is great for debugging, i.e., (1) there is a one-to-one correspondence between the layers in the symbolic and compiled circuit in PyTorch, and (2) one can easily retrieve the inputs to each layer and investigate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b8aefcdf-2515-4609-add0-be2a79efb819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchCircuit(\n",
      "  (0): TorchCategoricalLayer(\n",
      "    folds: 784  channels: 1  variables: 1  output-units: 4\n",
      "    input-shape: (784, 1, -1, 1)\n",
      "    output-shape: (784, -1, 4)\n",
      "    (probs): TorchParameter(\n",
      "      shape: (784, 4, 1, 256)\n",
      "      (0): TorchTensorParameter(output-shape: (784, 4, 1, 256))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(784, 4, 1, 256)]\n",
      "        output-shape: (784, 4, 1, 256)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): TorchTuckerLayer(\n",
      "    folds: 784  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (784, 2, -1, 4)\n",
      "    output-shape: (784, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (784, 4, 16)\n",
      "      (0): TorchTensorParameter(output-shape: (784, 4, 16))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(784, 4, 16)]\n",
      "        output-shape: (784, 4, 16)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): TorchTuckerLayer(\n",
      "    folds: 392  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (392, 2, -1, 4)\n",
      "    output-shape: (392, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (392, 4, 16)\n",
      "      (0): TorchTensorParameter(output-shape: (392, 4, 16))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(392, 4, 16)]\n",
      "        output-shape: (392, 4, 16)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): TorchMixingLayer(\n",
      "    folds: 196  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (196, 2, -1, 4)\n",
      "    output-shape: (196, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (196, 4, 2)\n",
      "      (0): TorchTensorParameter(output-shape: (196, 4, 2))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(196, 4, 2)]\n",
      "        output-shape: (196, 4, 2)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): TorchTuckerLayer(\n",
      "    folds: 196  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (196, 2, -1, 4)\n",
      "    output-shape: (196, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (196, 4, 16)\n",
      "      (0): TorchTensorParameter(output-shape: (196, 4, 16))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(196, 4, 16)]\n",
      "        output-shape: (196, 4, 16)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): TorchTuckerLayer(\n",
      "    folds: 98  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (98, 2, -1, 4)\n",
      "    output-shape: (98, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (98, 4, 16)\n",
      "      (0): TorchTensorParameter(output-shape: (98, 4, 16))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(98, 4, 16)]\n",
      "        output-shape: (98, 4, 16)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): TorchMixingLayer(\n",
      "    folds: 49  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (49, 2, -1, 4)\n",
      "    output-shape: (49, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (49, 4, 2)\n",
      "      (0): TorchTensorParameter(output-shape: (49, 4, 2))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(49, 4, 2)]\n",
      "        output-shape: (49, 4, 2)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): TorchTuckerLayer(\n",
      "    folds: 42  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (42, 2, -1, 4)\n",
      "    output-shape: (42, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (42, 4, 16)\n",
      "      (0): TorchTensorParameter(output-shape: (42, 4, 16))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(42, 4, 16)]\n",
      "        output-shape: (42, 4, 16)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): TorchTuckerLayer(\n",
      "    folds: 22  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (22, 2, -1, 4)\n",
      "    output-shape: (22, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (22, 4, 16)\n",
      "      (0): TorchTensorParameter(output-shape: (22, 4, 16))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(22, 4, 16)]\n",
      "        output-shape: (22, 4, 16)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): TorchMixingLayer(\n",
      "    folds: 9  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (9, 2, -1, 4)\n",
      "    output-shape: (9, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (9, 4, 2)\n",
      "      (0): TorchTensorParameter(output-shape: (9, 4, 2))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(9, 4, 2)]\n",
      "        output-shape: (9, 4, 2)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): TorchTuckerLayer(\n",
      "    folds: 12  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (12, 2, -1, 4)\n",
      "    output-shape: (12, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (12, 4, 16)\n",
      "      (0): TorchTensorParameter(output-shape: (12, 4, 16))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(12, 4, 16)]\n",
      "        output-shape: (12, 4, 16)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): TorchTuckerLayer(\n",
      "    folds: 8  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (8, 2, -1, 4)\n",
      "    output-shape: (8, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (8, 4, 16)\n",
      "      (0): TorchTensorParameter(output-shape: (8, 4, 16))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(8, 4, 16)]\n",
      "        output-shape: (8, 4, 16)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): TorchMixingLayer(\n",
      "    folds: 4  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (4, 2, -1, 4)\n",
      "    output-shape: (4, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (4, 4, 2)\n",
      "      (0): TorchTensorParameter(output-shape: (4, 4, 2))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(4, 4, 2)]\n",
      "        output-shape: (4, 4, 2)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (13): TorchTuckerLayer(\n",
      "    folds: 4  arity: 2  input-units: 4  output-units: 4\n",
      "    input-shape: (4, 2, -1, 4)\n",
      "    output-shape: (4, -1, 4)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (4, 4, 16)\n",
      "      (0): TorchTensorParameter(output-shape: (4, 4, 16))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(4, 4, 16)]\n",
      "        output-shape: (4, 4, 16)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (14): TorchTuckerLayer(\n",
      "    folds: 2  arity: 2  input-units: 4  output-units: 1\n",
      "    input-shape: (2, 2, -1, 4)\n",
      "    output-shape: (2, -1, 1)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (2, 1, 16)\n",
      "      (0): TorchTensorParameter(output-shape: (2, 1, 16))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(2, 1, 16)]\n",
      "        output-shape: (2, 1, 16)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (15): TorchMixingLayer(\n",
      "    folds: 1  arity: 2  input-units: 1  output-units: 1\n",
      "    input-shape: (1, 2, -1, 1)\n",
      "    output-shape: (1, -1, 1)\n",
      "    (weight): TorchParameter(\n",
      "      shape: (1, 1, 2)\n",
      "      (0): TorchTensorParameter(output-shape: (1, 1, 2))\n",
      "      (1): TorchSoftmaxParameter(\n",
      "        input-shapes: [(1, 1, 2)]\n",
      "        output-shape: (1, 1, 2)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5a2bd-4517-4c91-ab09-375998e5095c",
   "metadata": {},
   "source": [
    "### Folding your Circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7fae1-68ae-4923-ada5-c1820f298201",
   "metadata": {},
   "source": [
    "TODO: we have many layers, but actually many of them can be fused together since they can be evaluated in parallel, this is folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "541f4112-8f11-42b2-934c-9fdd3e036f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 4163\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of layers: {len(circuit.layers)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62766b-d0c9-46b3-96d7-4a12617330d3",
   "metadata": {},
   "source": [
    "TODO: let's now enable folding as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e42a5023-7147-41d9-919e-72e0552b1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = PipelineContext(\n",
    "    backend='torch',      # Use the PyTorch backend\n",
    "    # Specify the backend compilation flags next\n",
    "    semiring='lse-sum',   # Use numerically-stable LogSumExp-Sum semiring\n",
    "    fold=True,            # <---- Enable folding\n",
    "    optimize=False,       # Disable layer optimizations (for now)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f737ee9b-dc7a-437e-877d-74c876df3c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 942 ms, sys: 19.8 ms, total: 961 ms\n",
      "Wall time: 948 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "circuit = ctx.compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16615fd-38e8-4b7f-b03b-8e30502ce965",
   "metadata": {},
   "source": [
    "TODO: note that the compilation procedure took more time, let's now check the number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d15f7585-8ea5-4a0b-a22d-9025d30cb3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 26\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of layers: {len(circuit.layers)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe5fa1-04fe-49b8-886d-13bfadf68af6",
   "metadata": {},
   "source": [
    "TODO: we have far fewer layers. They have not disappeared, but they have been fused together. In fact the first layer has many folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fb933237-a1a6-4ca5-a6b9-35fef9afddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the first layer: TorchCategoricalLayer\n",
      "Number of folded layer within the first layer: 784\n"
     ]
    }
   ],
   "source": [
    "first_folded_layer = next(circuit.topological_ordering())\n",
    "print(f'Type of the first layer: {first_folded_layer.__class__.__name__}')\n",
    "print(f'Number of folded layer within the first layer: {first_folded_layer.num_folds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074e168-dee4-4234-8eae-afd28fae317f",
   "metadata": {},
   "source": [
    "TODO: all categoricals have been been fused in a single one, this drastically improves efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ad419f94-c4d6-4051-a817-5bb8b85382ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 421 ms, sys: 284 ms, total: 705 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch = torch.randint(256, size=(256, 1, 784), device=device)\n",
    "circuit(batch)\n",
    "if 'cuda' in str(device):\n",
    "    torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcdd8b3-96b1-4a08-97d1-77b4dbe7e8d8",
   "metadata": {},
   "source": [
    "TODO: we have achieved a ~xx.x speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13286bb-afa1-4284-8087-04fba7d65289",
   "metadata": {},
   "source": [
    "### Optimizing the Circuit Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8d4b6-53e7-4467-8469-02c15de93f70",
   "metadata": {},
   "source": [
    "TODO: the layers circuit we have built can be optimized. For instance there are kronecker layers that are espensive memory-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "91122716-845b-488a-a747-1d0972c181fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TorchCategoricalLayer', 'TorchKroneckerLayer', 'TorchDenseLayer', 'TorchKroneckerLayer', 'TorchDenseLayer', 'TorchMixingLayer', 'TorchKroneckerLayer', 'TorchDenseLayer', 'TorchKroneckerLayer', 'TorchDenseLayer', 'TorchMixingLayer', 'TorchKroneckerLayer', 'TorchDenseLayer', 'TorchKroneckerLayer', 'TorchDenseLayer', 'TorchMixingLayer', 'TorchKroneckerLayer', 'TorchDenseLayer', 'TorchKroneckerLayer', 'TorchDenseLayer', 'TorchMixingLayer', 'TorchKroneckerLayer', 'TorchDenseLayer', 'TorchKroneckerLayer', 'TorchDenseLayer', 'TorchMixingLayer']\n"
     ]
    }
   ],
   "source": [
    "print([layer.__class__.__name__ for layer in circuit.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3873403-7c97-4932-aeda-12d45e9b679e",
   "metadata": {},
   "source": [
    "TODO: mention EiNets paper, we can optimize this circuit by fusing kronecker product layers with dense layers. This and other optimizations are performed automatically regardless of the circuit structure, we just need to activate the flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aed662c8-79b5-4e05-8564-6e65b47a0478",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = PipelineContext(\n",
    "    backend='torch',      # Use the PyTorch backend\n",
    "    # Specify the backend compilation flags next\n",
    "    semiring='lse-sum',   # Specify how to evaluate sum and product layers\n",
    "                          # In this case we use the numerically-stable LogSumExp-Sum semiring (R, +, *),\n",
    "                          # where: + is the log-sum-exp operation, and * is the sum operation.\n",
    "    fold=True,            # Enable folding\n",
    "    optimize=True,        # <---- Enable layer optimizations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "05afb743-3489-45ff-9e1e-9460828ee808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 833 ms, sys: 14.8 ms, total: 848 ms\n",
      "Wall time: 833 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "circuit = ctx.compile(symbolic_circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e482ab-56db-41f5-9c34-c4d28eba6ac9",
   "metadata": {},
   "source": [
    "TODO: The compilation took even longer. If we look at the layers we find they are Tucker layers, which are much more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0c1fcf6-9f37-411e-ac3e-ddc8c5376194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TorchCategoricalLayer', 'TorchTuckerLayer', 'TorchTuckerLayer', 'TorchMixingLayer', 'TorchTuckerLayer', 'TorchTuckerLayer', 'TorchMixingLayer', 'TorchTuckerLayer', 'TorchTuckerLayer', 'TorchMixingLayer', 'TorchTuckerLayer', 'TorchTuckerLayer', 'TorchMixingLayer', 'TorchTuckerLayer', 'TorchTuckerLayer', 'TorchMixingLayer']\n"
     ]
    }
   ],
   "source": [
    "print([layer.__class__.__name__ for layer in circuit.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94aa778-d56c-44ac-8556-be0ce247335a",
   "metadata": {},
   "source": [
    "TODO: Let's now benchmark our circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "58dbcb48-e26a-4df3-87d0-7be6daa41ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 ms ± 8.16 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch = torch.randint(256, size=(256, 1, 784), device=device)\n",
    "circuit(batch)\n",
    "if 'cuda' in str(device):\n",
    "    torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d95c02-2c66-4414-b676-0dec303f2aa9",
   "metadata": {},
   "source": [
    "TODO: we have achieved ~x.xx speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed0fc0-31b9-428a-ab51-81e7ad7f2e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
